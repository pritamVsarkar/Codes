{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the audio book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "np.random.seed(9)\n",
    "raw_csv_data=np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "unscalled_inputs_all=raw_csv_data[:,1:-1] #except the 1st and last cloumn\n",
    "targets_all=raw_csv_data[:,-1] #last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets=list(targets_all).count(1) #number of ones in target\n",
    "zero_target_counter=0\n",
    "indices_to_remove=[]\n",
    "#keeping as many as 0s as 1s (we will delete others)\n",
    "#if number of zeros == number of ones then remove all further positions which contains zero value in targets\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i]==0:\n",
    "        zero_target_counter += 1\n",
    "        if zero_target_counter>num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "unscalled_inputs_equal_priors=np.delete(unscalled_inputs_all,indices_to_remove,axis=0) #axis 0 of the indices_to_remove vector\n",
    "targets_equal_priors=np.delete(targets_all,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs=preprocessing.scale(unscalled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffel the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_scaled_inputs=list(np.arange(scaled_inputs.shape[0]))\n",
    "np.random.shuffle(indices_of_scaled_inputs) #shuffleing the indices\n",
    "\n",
    "shuffled_inputs=scaled_inputs[indices_of_scaled_inputs]\n",
    "shuffled_targets=targets_equal_priors[indices_of_scaled_inputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training,validation &testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count=shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count=int(0.8*samples_count)\n",
    "validation_samples_count=int(0.1*samples_count)\n",
    "test_samples_count=samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs=shuffled_inputs[:train_samples_count] # 1-80\n",
    "train_targets=shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs=shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count] # 81-90\n",
    "validation_targets=shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs=shuffled_inputs[train_samples_count+validation_samples_count:]#91-100\n",
    "test_targets=shuffled_targets[train_samples_count+validation_samples_count:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the 3 datasets in *npz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create methods that handles batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Audiobooks_Data_Reader():\n",
    "    #dataset mandetory batch_size optional #dataset inputs+targets #dataset=train or validation or test \n",
    "    def __init__(self,dataset,batch_size=None): ### taking train or validation or test as input ###\n",
    "        \n",
    "        #dataset=train or validation or test\n",
    "        npz=np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        self.inputs,self.targets=npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "        \n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "        ### creating number of batches wrt input\n",
    "\n",
    "    # A method which loads the next batch\n",
    "    def __next__(self): ###iterating each batch at a time ####\n",
    "        \n",
    "        if self.curr_batch >= self.batch_count:### iteration termination logic ###\n",
    "                self.curr_batch = 0\n",
    "                raise StopIteration()\n",
    "       \n",
    "        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n",
    "        #get each slice in each iteration\n",
    "        ### creating each batch slice ###\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size) #each slice size\n",
    "        batch_inputs = self.inputs[batch_slice]\n",
    "        batch_targets = self.targets[batch_slice]\n",
    "        \n",
    "        self.curr_batch += 1\n",
    "        \n",
    "        ### hot encoding ###\n",
    "        classes_num = 2\n",
    "        ### list of all zeroes of length batch_targets.shape[0] ###\n",
    "        targets_one_hot = np.zeros((batch_targets.shape[0], classes_num)) \n",
    "        ## we can use 1st 3 lines of next batch ##\n",
    "        targets_one_hot[range(batch_targets.shape[0]), batch_targets] = 1 #batch_targets 0 or 1 #******\n",
    "\n",
    "        return batch_inputs, targets_one_hot ### returning batched inputs and one hot encoded batched output ###\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            (batch_targets.shape[0]=batched_target_length)0    1    \\n                                    .................................\\n                                            0             0    0 \\n                                            1             0    0 \\n                                            2             0    0 \\n                                            .             .\\n                                            .             .\\n                                            n             0    0 \\n                                            \\n            (batch_targets.shape[0]=batched_target_length)0    1    i  batch_targets[i]\\n                                    ................................................\\n                                            0             0    1 <=[0,      1]              =1\\n                                            1             1    0 <=[1,      0]              =1\\n                                            2             1    0 <=[2,      0]              =1\\n                                            .             .\\n                                            .             .\\n                                            n             0    1 <=>[n,     1]              =1\\n                \\n                for i in range(batch_targets.shape[0]):\\n                    all_zeroes[i,batch_targets]=1\\n                    \\n                means when batch_targets=1 at i=k instance =>all_zeroes[k,1]=1 || 0 encoded to >1[k 0] 0[k 1]\\n                and when batch_targets=0 at i=l instance   =>all_zeroes[l,0]=0 || 1 encoded to >0[l 0] 1[l 1]\\n        \\n       \\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### We can also use these 3 lines\n",
    "#******\n",
    "### means\n",
    "'''\n",
    "            (batch_targets.shape[0]=batched_target_length)0    1    \n",
    "                                    .................................\n",
    "                                            0             0    0 \n",
    "                                            1             0    0 \n",
    "                                            2             0    0 \n",
    "                                            .             .\n",
    "                                            .             .\n",
    "                                            n             0    0 \n",
    "                                            \n",
    "            (batch_targets.shape[0]=batched_target_length)0    1    i  batch_targets[i]\n",
    "                                    ................................................\n",
    "                                            0             0    1 <=[0,      1]              =1\n",
    "                                            1             1    0 <=[1,      0]              =1\n",
    "                                            2             1    0 <=[2,      0]              =1\n",
    "                                            .             .\n",
    "                                            .             .\n",
    "                                            n             0    1 <=>[n,     1]              =1\n",
    "                \n",
    "                for i in range(batch_targets.shape[0]):\n",
    "                    all_zeroes[i,batch_targets]=1\n",
    "                    \n",
    "                means when batch_targets=1 at i=k instance =>all_zeroes[k,1]=1 || 0 encoded to >1[k 0] 0[k 1]\n",
    "                and when batch_targets=0 at i=l instance   =>all_zeroes[l,0]=0 || 1 encoded to >0[l 0] 1[l 1]\n",
    "        \n",
    "       \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.8363. Validation loss: 0.7723. Validation accuracy: 52.796%\n",
      "Epoch 2. Mean loss: 0.7070. Validation loss: 0.6802. Validation accuracy: 58.166%\n",
      "Epoch 3. Mean loss: 0.6364. Validation loss: 0.6184. Validation accuracy: 61.074%\n",
      "Epoch 4. Mean loss: 0.5890. Validation loss: 0.5731. Validation accuracy: 63.982%\n",
      "Epoch 5. Mean loss: 0.5545. Validation loss: 0.5401. Validation accuracy: 66.443%\n",
      "Epoch 6. Mean loss: 0.5277. Validation loss: 0.5153. Validation accuracy: 71.141%\n",
      "Epoch 7. Mean loss: 0.5065. Validation loss: 0.4958. Validation accuracy: 73.154%\n",
      "Epoch 8. Mean loss: 0.4888. Validation loss: 0.4800. Validation accuracy: 74.720%\n",
      "Epoch 9. Mean loss: 0.4739. Validation loss: 0.4668. Validation accuracy: 75.615%\n",
      "Epoch 10. Mean loss: 0.4612. Validation loss: 0.4554. Validation accuracy: 75.839%\n",
      "Epoch 11. Mean loss: 0.4502. Validation loss: 0.4457. Validation accuracy: 76.063%\n",
      "Epoch 12. Mean loss: 0.4406. Validation loss: 0.4372. Validation accuracy: 76.510%\n",
      "Epoch 13. Mean loss: 0.4321. Validation loss: 0.4295. Validation accuracy: 76.957%\n",
      "Epoch 14. Mean loss: 0.4246. Validation loss: 0.4228. Validation accuracy: 77.852%\n",
      "Epoch 15. Mean loss: 0.4180. Validation loss: 0.4168. Validation accuracy: 77.629%\n",
      "Epoch 16. Mean loss: 0.4120. Validation loss: 0.4114. Validation accuracy: 77.852%\n",
      "Epoch 17. Mean loss: 0.4067. Validation loss: 0.4065. Validation accuracy: 78.971%\n",
      "Epoch 18. Mean loss: 0.4018. Validation loss: 0.4020. Validation accuracy: 79.195%\n",
      "Epoch 19. Mean loss: 0.3974. Validation loss: 0.3979. Validation accuracy: 79.195%\n",
      "Epoch 20. Mean loss: 0.3932. Validation loss: 0.3941. Validation accuracy: 79.195%\n",
      "Epoch 21. Mean loss: 0.3894. Validation loss: 0.3905. Validation accuracy: 79.195%\n",
      "Epoch 22. Mean loss: 0.3859. Validation loss: 0.3872. Validation accuracy: 79.195%\n",
      "Epoch 23. Mean loss: 0.3828. Validation loss: 0.3842. Validation accuracy: 78.747%\n",
      "Epoch 24. Mean loss: 0.3799. Validation loss: 0.3814. Validation accuracy: 78.523%\n",
      "Epoch 25. Mean loss: 0.3772. Validation loss: 0.3788. Validation accuracy: 78.747%\n",
      "Epoch 26. Mean loss: 0.3747. Validation loss: 0.3764. Validation accuracy: 79.195%\n",
      "Epoch 27. Mean loss: 0.3724. Validation loss: 0.3742. Validation accuracy: 79.642%\n",
      "Epoch 28. Mean loss: 0.3702. Validation loss: 0.3721. Validation accuracy: 79.642%\n",
      "Epoch 29. Mean loss: 0.3682. Validation loss: 0.3701. Validation accuracy: 79.642%\n",
      "Epoch 30. Mean loss: 0.3662. Validation loss: 0.3683. Validation accuracy: 79.642%\n",
      "Epoch 31. Mean loss: 0.3644. Validation loss: 0.3665. Validation accuracy: 79.866%\n",
      "Epoch 32. Mean loss: 0.3627. Validation loss: 0.3649. Validation accuracy: 79.866%\n",
      "Epoch 33. Mean loss: 0.3610. Validation loss: 0.3634. Validation accuracy: 80.089%\n",
      "Epoch 34. Mean loss: 0.3595. Validation loss: 0.3620. Validation accuracy: 80.089%\n",
      "Epoch 35. Mean loss: 0.3580. Validation loss: 0.3606. Validation accuracy: 80.313%\n",
      "Epoch 36. Mean loss: 0.3566. Validation loss: 0.3592. Validation accuracy: 80.537%\n",
      "Epoch 37. Mean loss: 0.3552. Validation loss: 0.3580. Validation accuracy: 80.537%\n",
      "Epoch 38. Mean loss: 0.3539. Validation loss: 0.3568. Validation accuracy: 80.537%\n",
      "Epoch 39. Mean loss: 0.3527. Validation loss: 0.3557. Validation accuracy: 80.537%\n",
      "Epoch 40. Mean loss: 0.3515. Validation loss: 0.3546. Validation accuracy: 80.537%\n",
      "Epoch 41. Mean loss: 0.3503. Validation loss: 0.3536. Validation accuracy: 80.537%\n",
      "Epoch 42. Mean loss: 0.3492. Validation loss: 0.3526. Validation accuracy: 80.537%\n",
      "Epoch 43. Mean loss: 0.3482. Validation loss: 0.3517. Validation accuracy: 81.208%\n",
      "Epoch 44. Mean loss: 0.3471. Validation loss: 0.3508. Validation accuracy: 80.984%\n",
      "Epoch 45. Mean loss: 0.3462. Validation loss: 0.3499. Validation accuracy: 80.984%\n",
      "Epoch 46. Mean loss: 0.3452. Validation loss: 0.3491. Validation accuracy: 80.984%\n",
      "Epoch 47. Mean loss: 0.3443. Validation loss: 0.3483. Validation accuracy: 81.208%\n",
      "Epoch 48. Mean loss: 0.3434. Validation loss: 0.3475. Validation accuracy: 81.432%\n",
      "Epoch 49. Mean loss: 0.3425. Validation loss: 0.3468. Validation accuracy: 81.432%\n",
      "Epoch 50. Mean loss: 0.3417. Validation loss: 0.3460. Validation accuracy: 81.432%\n",
      "Epoch 51. Mean loss: 0.3409. Validation loss: 0.3454. Validation accuracy: 81.879%\n",
      "Epoch 52. Mean loss: 0.3401. Validation loss: 0.3447. Validation accuracy: 81.879%\n",
      "Epoch 53. Mean loss: 0.3393. Validation loss: 0.3440. Validation accuracy: 81.879%\n",
      "Epoch 54. Mean loss: 0.3386. Validation loss: 0.3434. Validation accuracy: 81.655%\n",
      "Epoch 55. Mean loss: 0.3378. Validation loss: 0.3429. Validation accuracy: 81.879%\n",
      "Epoch 56. Mean loss: 0.3371. Validation loss: 0.3423. Validation accuracy: 81.879%\n",
      "Epoch 57. Mean loss: 0.3365. Validation loss: 0.3418. Validation accuracy: 81.655%\n",
      "Epoch 58. Mean loss: 0.3358. Validation loss: 0.3413. Validation accuracy: 81.655%\n",
      "Epoch 59. Mean loss: 0.3351. Validation loss: 0.3408. Validation accuracy: 81.879%\n",
      "Epoch 60. Mean loss: 0.3345. Validation loss: 0.3403. Validation accuracy: 81.879%\n",
      "Epoch 61. Mean loss: 0.3339. Validation loss: 0.3399. Validation accuracy: 82.103%\n",
      "Epoch 62. Mean loss: 0.3333. Validation loss: 0.3394. Validation accuracy: 82.327%\n",
      "Epoch 63. Mean loss: 0.3327. Validation loss: 0.3389. Validation accuracy: 82.103%\n",
      "Epoch 64. Mean loss: 0.3321. Validation loss: 0.3385. Validation accuracy: 81.879%\n",
      "Epoch 65. Mean loss: 0.3316. Validation loss: 0.3381. Validation accuracy: 81.879%\n",
      "Epoch 66. Mean loss: 0.3310. Validation loss: 0.3377. Validation accuracy: 81.432%\n",
      "Epoch 67. Mean loss: 0.3305. Validation loss: 0.3373. Validation accuracy: 81.432%\n",
      "Epoch 68. Mean loss: 0.3300. Validation loss: 0.3369. Validation accuracy: 81.432%\n",
      "Epoch 69. Mean loss: 0.3295. Validation loss: 0.3366. Validation accuracy: 81.879%\n",
      "Epoch 70. Mean loss: 0.3290. Validation loss: 0.3363. Validation accuracy: 81.879%\n",
      "Epoch 71. Mean loss: 0.3285. Validation loss: 0.3360. Validation accuracy: 81.879%\n",
      "Epoch 72. Mean loss: 0.3281. Validation loss: 0.3356. Validation accuracy: 81.879%\n",
      "Epoch 73. Mean loss: 0.3276. Validation loss: 0.3353. Validation accuracy: 82.103%\n",
      "Epoch 74. Mean loss: 0.3272. Validation loss: 0.3351. Validation accuracy: 81.879%\n",
      "Epoch 75. Mean loss: 0.3267. Validation loss: 0.3348. Validation accuracy: 81.879%\n",
      "Epoch 76. Mean loss: 0.3263. Validation loss: 0.3345. Validation accuracy: 81.879%\n",
      "Epoch 77. Mean loss: 0.3259. Validation loss: 0.3343. Validation accuracy: 81.879%\n",
      "Epoch 78. Mean loss: 0.3255. Validation loss: 0.3340. Validation accuracy: 81.879%\n",
      "Epoch 79. Mean loss: 0.3251. Validation loss: 0.3338. Validation accuracy: 82.103%\n",
      "Epoch 80. Mean loss: 0.3247. Validation loss: 0.3336. Validation accuracy: 82.103%\n",
      "Epoch 81. Mean loss: 0.3243. Validation loss: 0.3334. Validation accuracy: 82.327%\n",
      "Epoch 82. Mean loss: 0.3240. Validation loss: 0.3332. Validation accuracy: 82.327%\n",
      "Epoch 83. Mean loss: 0.3236. Validation loss: 0.3330. Validation accuracy: 82.327%\n",
      "Epoch 84. Mean loss: 0.3232. Validation loss: 0.3328. Validation accuracy: 82.327%\n",
      "Epoch 85. Mean loss: 0.3229. Validation loss: 0.3326. Validation accuracy: 82.327%\n",
      "Epoch 86. Mean loss: 0.3225. Validation loss: 0.3324. Validation accuracy: 82.327%\n",
      "Epoch 87. Mean loss: 0.3222. Validation loss: 0.3323. Validation accuracy: 82.327%\n",
      "Epoch 88. Mean loss: 0.3219. Validation loss: 0.3321. Validation accuracy: 82.327%\n",
      "Epoch 89. Mean loss: 0.3215. Validation loss: 0.3319. Validation accuracy: 82.327%\n",
      "Epoch 90. Mean loss: 0.3212. Validation loss: 0.3318. Validation accuracy: 82.327%\n",
      "Epoch 91. Mean loss: 0.3209. Validation loss: 0.3316. Validation accuracy: 82.327%\n",
      "Epoch 92. Mean loss: 0.3206. Validation loss: 0.3314. Validation accuracy: 82.327%\n",
      "Epoch 93. Mean loss: 0.3203. Validation loss: 0.3313. Validation accuracy: 82.327%\n",
      "Epoch 94. Mean loss: 0.3200. Validation loss: 0.3311. Validation accuracy: 82.327%\n",
      "Epoch 95. Mean loss: 0.3197. Validation loss: 0.3310. Validation accuracy: 82.327%\n",
      "Epoch 96. Mean loss: 0.3194. Validation loss: 0.3308. Validation accuracy: 82.550%\n",
      "Epoch 97. Mean loss: 0.3191. Validation loss: 0.3307. Validation accuracy: 82.550%\n",
      "Epoch 98. Mean loss: 0.3188. Validation loss: 0.3305. Validation accuracy: 82.550%\n",
      "Epoch 99. Mean loss: 0.3186. Validation loss: 0.3304. Validation accuracy: 82.550%\n",
      "Epoch 100. Mean loss: 0.3183. Validation loss: 0.3303. Validation accuracy: 82.550%\n",
      "Epoch 101. Mean loss: 0.3180. Validation loss: 0.3301. Validation accuracy: 82.550%\n",
      "Epoch 102. Mean loss: 0.3178. Validation loss: 0.3300. Validation accuracy: 82.550%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103. Mean loss: 0.3175. Validation loss: 0.3299. Validation accuracy: 82.550%\n",
      "Epoch 104. Mean loss: 0.3173. Validation loss: 0.3298. Validation accuracy: 82.550%\n",
      "Epoch 105. Mean loss: 0.3170. Validation loss: 0.3296. Validation accuracy: 82.550%\n",
      "Epoch 106. Mean loss: 0.3168. Validation loss: 0.3295. Validation accuracy: 82.550%\n",
      "Epoch 107. Mean loss: 0.3166. Validation loss: 0.3294. Validation accuracy: 82.550%\n",
      "Epoch 108. Mean loss: 0.3163. Validation loss: 0.3293. Validation accuracy: 82.550%\n",
      "Epoch 109. Mean loss: 0.3161. Validation loss: 0.3292. Validation accuracy: 82.327%\n",
      "Epoch 110. Mean loss: 0.3159. Validation loss: 0.3291. Validation accuracy: 82.103%\n",
      "Epoch 111. Mean loss: 0.3156. Validation loss: 0.3290. Validation accuracy: 82.327%\n",
      "Epoch 112. Mean loss: 0.3154. Validation loss: 0.3289. Validation accuracy: 82.327%\n",
      "Epoch 113. Mean loss: 0.3152. Validation loss: 0.3288. Validation accuracy: 82.103%\n",
      "Epoch 114. Mean loss: 0.3150. Validation loss: 0.3287. Validation accuracy: 82.327%\n",
      "Epoch 115. Mean loss: 0.3148. Validation loss: 0.3287. Validation accuracy: 82.327%\n",
      "Epoch 116. Mean loss: 0.3146. Validation loss: 0.3286. Validation accuracy: 82.327%\n",
      "Epoch 117. Mean loss: 0.3144. Validation loss: 0.3285. Validation accuracy: 82.327%\n",
      "Epoch 118. Mean loss: 0.3142. Validation loss: 0.3284. Validation accuracy: 82.327%\n",
      "Epoch 119. Mean loss: 0.3140. Validation loss: 0.3284. Validation accuracy: 82.327%\n",
      "Epoch 120. Mean loss: 0.3138. Validation loss: 0.3283. Validation accuracy: 82.550%\n",
      "Epoch 121. Mean loss: 0.3136. Validation loss: 0.3282. Validation accuracy: 82.327%\n",
      "Epoch 122. Mean loss: 0.3134. Validation loss: 0.3281. Validation accuracy: 82.774%\n",
      "Epoch 123. Mean loss: 0.3132. Validation loss: 0.3281. Validation accuracy: 82.774%\n",
      "Epoch 124. Mean loss: 0.3130. Validation loss: 0.3280. Validation accuracy: 82.550%\n",
      "Epoch 125. Mean loss: 0.3129. Validation loss: 0.3280. Validation accuracy: 82.550%\n",
      "Epoch 126. Mean loss: 0.3127. Validation loss: 0.3279. Validation accuracy: 83.221%\n",
      "Epoch 127. Mean loss: 0.3125. Validation loss: 0.3278. Validation accuracy: 83.445%\n",
      "Epoch 128. Mean loss: 0.3123. Validation loss: 0.3277. Validation accuracy: 83.445%\n",
      "Epoch 129. Mean loss: 0.3122. Validation loss: 0.3277. Validation accuracy: 83.221%\n",
      "Epoch 130. Mean loss: 0.3120. Validation loss: 0.3277. Validation accuracy: 83.221%\n",
      "Epoch 131. Mean loss: 0.3118. Validation loss: 0.3276. Validation accuracy: 83.221%\n",
      "Epoch 132. Mean loss: 0.3116. Validation loss: 0.3275. Validation accuracy: 83.221%\n",
      "Epoch 133. Mean loss: 0.3115. Validation loss: 0.3275. Validation accuracy: 83.445%\n",
      "Epoch 134. Mean loss: 0.3113. Validation loss: 0.3275. Validation accuracy: 83.445%\n",
      "Epoch 135. Mean loss: 0.3111. Validation loss: 0.3274. Validation accuracy: 83.445%\n",
      "Epoch 136. Mean loss: 0.3110. Validation loss: 0.3274. Validation accuracy: 83.445%\n",
      "Epoch 137. Mean loss: 0.3108. Validation loss: 0.3273. Validation accuracy: 83.445%\n",
      "Epoch 138. Mean loss: 0.3106. Validation loss: 0.3273. Validation accuracy: 83.445%\n",
      "Epoch 139. Mean loss: 0.3104. Validation loss: 0.3273. Validation accuracy: 83.445%\n",
      "Epoch 140. Mean loss: 0.3103. Validation loss: 0.3272. Validation accuracy: 83.445%\n",
      "Epoch 141. Mean loss: 0.3101. Validation loss: 0.3272. Validation accuracy: 83.445%\n",
      "Epoch 142. Mean loss: 0.3100. Validation loss: 0.3272. Validation accuracy: 83.445%\n",
      "Epoch 143. Mean loss: 0.3098. Validation loss: 0.3271. Validation accuracy: 83.445%\n",
      "Epoch 144. Mean loss: 0.3097. Validation loss: 0.3271. Validation accuracy: 83.445%\n",
      "end of training\n"
     ]
    }
   ],
   "source": [
    "input_size=10\n",
    "output_size=2\n",
    "hidden_layer_size=100\n",
    "\n",
    "#removes all previous stored graphs or data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "inputs=tf.placeholder(tf.float32,[None,input_size])\n",
    "targets=tf.placeholder(tf.int32,[None,output_size])\n",
    "\n",
    "#for layer 1\n",
    "weights_1=tf.get_variable('weights_1',[input_size,hidden_layer_size])\n",
    "biases_1=tf.get_variable('biases_1',[hidden_layer_size])\n",
    "\n",
    "outputs_1=tf.nn.relu(tf.matmul(inputs,weights_1)+biases_1) #layer one activation function relu\n",
    "\n",
    "#for layer 2\n",
    "weights_2=tf.get_variable('weights_2',[hidden_layer_size,hidden_layer_size])\n",
    "biases_2=tf.get_variable('biases_2',[hidden_layer_size])\n",
    "\n",
    "outputs_2=tf.nn.relu(tf.matmul(outputs_1,weights_2)+biases_2) #layer two activation function relu\n",
    "\n",
    "#final layer \n",
    "weights_3=tf.get_variable('weights_3',[hidden_layer_size,output_size])\n",
    "biases_3=tf.get_variable('biases_3',[output_size])\n",
    "\n",
    "outputs=tf.matmul(outputs_2,weights_3)+biases_3 #output without activation function\n",
    "\n",
    "\n",
    "#1st logit function then softmax then cross_entropy loss calculation\n",
    "#numaricaly stable function when we deals with very small nnumbers\n",
    "cross_entropy_loss=tf.nn.softmax_cross_entropy_with_logits(logits=outputs,labels=targets) #returns list of losses\n",
    "mean_loss=tf.reduce_mean(cross_entropy_loss) #<<<<<<<<<<\n",
    "\n",
    "#optimization method  Adam\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001).minimize(mean_loss)#back_prob also performed here #<<<<<<<<<\n",
    "\n",
    "#prediction accuracy\n",
    "#equal returns 1 or 0 when values matched or not matched respectively\n",
    "#output_equals_target is a vactor of 0's (missmatch) and 1's (match) \n",
    "#argmax returs the index(r,c) of the maximum value for each target and output list, we need only the column number \n",
    "output_equals_target=tf.equal(tf.arg_max(outputs,1),tf.arg_max(targets,1))#argmax retruns row &col num here 1 means only cloumn\n",
    "accuracy=tf.reduce_mean(tf.cast(output_equals_target,tf.float32)) #<<<<< #cast->[0,1,1]=>[0.0,1.0,1.0]\n",
    "\n",
    "#creating sessions\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "#initialize all variables \n",
    "initializer=tf.global_variables_initializer() #<<<<<<<\n",
    "sess.run(initializer)\n",
    "\n",
    "#batching total 100 batches required\n",
    "batch_size=100 #batches of max 100 entries\n",
    "\n",
    "\n",
    "max_epochs=500\n",
    "\n",
    "previous_validation_loss=9999999. #. makes an int to float \n",
    "\n",
    "train_data=Audiobooks_Data_Reader('train',batch_size)\n",
    "validation_data=Audiobooks_Data_Reader('validation')\n",
    "\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    current_epoch_loss=0. #trainig loss \n",
    "    total_batch_loss=0.\n",
    "    \n",
    "    for batch_inputs,batch_targets in train_data:\n",
    "        \n",
    "        #getting minimized batch loss with backpropagation\n",
    "        _,current_batch_loss=sess.run([optimizer,mean_loss],\n",
    "                                      feed_dict={inputs:batch_inputs,targets:batch_targets}) \n",
    "        \n",
    "        total_batch_loss += current_batch_loss\n",
    "        \n",
    "    current_epoch_loss = total_batch_loss/train_data.batch_count\n",
    "    \n",
    "    #validation after each epoch\n",
    "    validation_loss,validation_accuracy=0,0\n",
    "    \n",
    "    for batch_inputs,batch_targets in validation_data:\n",
    "        \n",
    "        validation_loss,validation_accuracy=sess.run([mean_loss,accuracy],\n",
    "                                                     feed_dict={inputs:batch_inputs,targets:batch_targets})\n",
    "    \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.4f}'.format(current_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.4f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.3f}'.format(validation_accuracy * 100.)+'%')\n",
    "    if validation_loss>previous_validation_loss:\n",
    "        break\n",
    "    previous_validation_loss=validation_loss\n",
    "print('end of training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 80.80% . Mean loss: 0.348\n"
     ]
    }
   ],
   "source": [
    "test_data=Audiobooks_Data_Reader('test')\n",
    "test_loss,test_accuracy=0,0\n",
    "    \n",
    "for batch_inputs,batch_targets in test_data:\n",
    "    test_loss,test_accuracy=sess.run([mean_loss,accuracy],\n",
    "                                     feed_dict={inputs:batch_inputs,targets:batch_targets})\n",
    "test_accuracy_percent = test_accuracy * 100.\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%'+' . Mean loss: '+'{0:.3f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
